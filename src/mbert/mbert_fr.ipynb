{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dZ9uCrkfVbhp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d2ae5051-db07-4c70-cc1e-3d0c469598a8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path 'NLP_final_project' already exists and is not an empty directory.\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.14.1)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.46)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.8.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.62.3)\n",
            "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.10.3)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.2.1)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (3.10.0.2)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.6)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.6.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.10.8)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.1.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n"
          ]
        }
      ],
      "source": [
        "  #install essential packages\n",
        "! git clone https://github.com/xyyzh/NLP_final_project.git\n",
        "!pip install transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xonggg9dVbhp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4d12f788-2126-4186-87da-c9d9b29b00dd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "#import\n",
        "import json\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import nltk\n",
        "import re\n",
        "import torch\n",
        "import gc\n",
        "import torch.nn as nn\n",
        "from sklearn.metrics import mean_absolute_error, classification_report, accuracy_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "from transformers import BertModel, BertTokenizer, AdamW, get_linear_schedule_with_warmup\n",
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
        "%matplotlib inline\n",
        "\n",
        "import pandas\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.tokenize import word_tokenize\n",
        "ps = PorterStemmer()\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "from nltk.tokenize import word_tokenize\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "from nltk.tokenize.treebank import TreebankWordDetokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "osyqMWVFVbhq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "34bc4c92-0bc6-4cc5-e478-49d1d28f85d8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "There are 1 GPU(s) available.\n",
            "Device name: Tesla P100-PCIE-16GB\n"
          ]
        }
      ],
      "source": [
        "#load the GPU\n",
        "if torch.cuda.is_available():       \n",
        "    device = torch.device(\"cuda\")\n",
        "    print(f'There are {torch.cuda.device_count()} GPU(s) available.')\n",
        "    print('Device name:', torch.cuda.get_device_name(0))\n",
        "\n",
        "else:\n",
        "    print('No GPU available, using the CPU instead.')\n",
        "    device = torch.device(\"cpu\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ywXfZ8qBVbhq"
      },
      "outputs": [],
      "source": [
        "#Training metadata\n",
        "epochs = 10\n",
        "batch_size=16"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lpZkvNd6wM-g"
      },
      "outputs": [],
      "source": [
        "def convert_train_data_of(lang):\n",
        "  if lang == 'en':\n",
        "    path = \"NLP_final_project/train/dataset_en_train_trimmed.json\"\n",
        "  elif lang == 'fr':\n",
        "    path = \"NLP_final_project/train/dataset_fr_train_trimmed.json\"\n",
        "  elif lang == 'de':\n",
        "    path = \"NLP_final_project/train/dataset_de_train_trimmed.json\"\n",
        "  elif lang == 'es':\n",
        "    path = \"NLP_final_project/train/dataset_es_train_trimmed.json\"\n",
        "  elif lang == 'zh':\n",
        "    path = \"NLP_final_project/train/dataset_zh_train_trimmed.json\"\n",
        "  elif lang == 'ja':\n",
        "    path = \"NLP_final_project/train/dataset_ja_train_trimmed.json\"\n",
        "  else:\n",
        "    print(\"Not a valid option\")\n",
        "    return -1\n",
        "\n",
        "  training = pd.read_json(path, lines=True)\n",
        "  training.drop([\"product_id\", \"reviewer_id\", \"language\", \"product_category\", \"review_title\"], inplace=True, axis=1)\n",
        "  Review = training.review_body.values\n",
        "\n",
        "  Star = [int(string) - 1 for string in training.stars.values]\n",
        "  train_text, val_text, train_labels, val_labels = train_test_split(Review, Star, random_state=2021, test_size=0.3, stratify=Star)\n",
        "  mlength = 100\n",
        "  # tokenize and encode sequences in the training set\n",
        "  tokens_train = tokenizer.batch_encode_plus(\n",
        "      train_text.tolist(),\n",
        "      max_length = mlength,\n",
        "      padding=True,\n",
        "      truncation=True\n",
        "  )\n",
        "\n",
        "# tokenize and encode sequences in the validation set\n",
        "  tokens_val = tokenizer.batch_encode_plus(\n",
        "      val_text.tolist(),\n",
        "      max_length = mlength,\n",
        "      padding=True,\n",
        "      truncation=True\n",
        "  )  \n",
        "\n",
        "  train_seq = torch.tensor(tokens_train['input_ids'])\n",
        "  train_mask = torch.tensor(tokens_train['attention_mask'])\n",
        "  train_y = torch.tensor(train_labels)\n",
        "\n",
        "  val_seq = torch.tensor(tokens_val['input_ids'])\n",
        "  val_mask = torch.tensor(tokens_val['attention_mask'])\n",
        "  val_y = torch.tensor(val_labels)\n",
        "\n",
        "  # wrap tensors\n",
        "  train_data = TensorDataset(train_seq, train_mask, train_y)\n",
        "\n",
        "  # sampler for sampling the data during training\n",
        "  train_sampler = RandomSampler(train_data)\n",
        "\n",
        "  # dataLoader for train set\n",
        "  train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
        "\n",
        "  # wrap tensors\n",
        "  val_data = TensorDataset(val_seq, val_mask, val_y)\n",
        "\n",
        "  # sampler for sampling the data during training\n",
        "  val_sampler = SequentialSampler(val_data)\n",
        "\n",
        "  # dataLoader for validation set\n",
        "  val_dataloader = DataLoader(val_data, sampler = val_sampler, batch_size=batch_size)\n",
        "\n",
        "  return train_dataloader, val_dataloader\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rmpOn6ihyHbU"
      },
      "outputs": [],
      "source": [
        "def convert_test_data_of(lang):\n",
        "  if lang == 'en':\n",
        "    path = \"NLP_final_project/test/dataset_en_test.json\"\n",
        "  elif lang == 'fr':\n",
        "    path = \"NLP_final_project/test/dataset_fr_test.json\"\n",
        "  elif lang == 'de':\n",
        "    path = \"NLP_final_project/test/dataset_de_test.json\"\n",
        "  elif lang == 'es':\n",
        "    path = \"NLP_final_project/test/dataset_es_test.json\"\n",
        "  elif lang == 'zh':\n",
        "    path = \"NLP_final_project/test/dataset_zh_test.json\"\n",
        "  elif lang == 'ja':\n",
        "    path = \"NLP_final_project/test/dataset_ja_test.json\"\n",
        "  else:\n",
        "    print(\"Not a valid option\")\n",
        "    return -1\n",
        "  test = pd.read_json(path, lines=True)\n",
        "  test.drop([\"product_id\", \"reviewer_id\", \"language\", \"product_category\", \"review_title\"], inplace=True, axis=1)\n",
        "  test_text=test.review_body.values\n",
        "  test_labels=[int(string) - 1 for string in test.stars.values]\n",
        "  mlength=100\n",
        "  tokens_test = tokenizer.batch_encode_plus(\n",
        "    test_text.tolist(),\n",
        "    max_length = mlength,\n",
        "    padding=True,\n",
        "    truncation=True\n",
        "  )\n",
        "\n",
        "  test_seq = torch.tensor(tokens_test['input_ids'])\n",
        "  test_mask = torch.tensor(tokens_test['attention_mask'])\n",
        "  test_y = torch.tensor(test_labels)\n",
        "\n",
        "  # wrap tensors\n",
        "  test_data = TensorDataset(test_seq, test_mask, test_y)\n",
        "\n",
        "  # sampler for sampling the data during training\n",
        "  test_sampler = SequentialSampler(test_data)\n",
        "\n",
        "  # dataLoader for validation set\n",
        "  test_dataloader = DataLoader(test_data, sampler = test_sampler, batch_size=batch_size)\n",
        "\n",
        "  return test_dataloader, test_y"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased', do_lower_case=True)\n",
        "train_dataloader, val_dataloader = convert_train_data_of('fr')"
      ],
      "metadata": {
        "id": "bwkORBdVeWeB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ji_74CR3RuYZ"
      },
      "outputs": [],
      "source": [
        "class BERT(nn.Module):\n",
        "    \n",
        "    def __init__(self, bert):\n",
        "      \n",
        "      super(BERT, self).__init__()\n",
        "\n",
        "      self.bert = bert \n",
        "      \n",
        "      self.classifier = nn.Sequential(\n",
        "          nn.Linear(768, 512),\n",
        "          nn.ReLU(),\n",
        "          nn.Linear(512, 5)\n",
        "      )\n",
        "\n",
        "    #define the forward pass\n",
        "    def forward(self, sent_id, mask):\n",
        "\n",
        "      #pass the inputs to the model  \n",
        "      outputs = self.bert(input_ids = sent_id, attention_mask = mask)\n",
        "      \n",
        "     # Extract the last hidden state of the token `[CLS]` for classification task\n",
        "      last_hidden_state_cls = outputs[0][:, 0, :]\n",
        "\n",
        "      # Feed input to classifier to compute logits\n",
        "      logits = self.classifier(last_hidden_state_cls)\n",
        "\n",
        "      return logits"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T6vRUz2C3fBB"
      },
      "outputs": [],
      "source": [
        "def load_pretrained_model():\n",
        "\n",
        "  bert=BertModel.from_pretrained('bert-base-multilingual-cased', return_dict=False)\n",
        "  model = BERT(bert)\n",
        "  '''\n",
        "  for param in bert.parameters():\n",
        "    param.requires_grad = False\n",
        "    # pass the pre-trained BERT to our define architecture\n",
        "  model = BERT_Arch(bert)\n",
        "  '''\n",
        "  # push the model to GPU\n",
        "\n",
        "  model = model.to(device)  \n",
        "  return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RfLvglvIVbhr",
        "outputId": "3a750966-e679-4383-f1e6-5a2d586a7450"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        }
      ],
      "source": [
        "#load tokenizer models\n",
        "\n",
        "\n",
        "model = load_pretrained_model()\n",
        "\n",
        "optimizer = AdamW(model.parameters(), lr = 2e-5)  \n",
        "\n",
        "cross_entropy  = nn.CrossEntropyLoss() \n",
        "\n",
        "\n",
        "scheduler = get_linear_schedule_with_warmup(optimizer,\n",
        "                                            num_warmup_steps=0, # Default value\n",
        "                                            num_training_steps=len(train_dataloader)*epochs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ID2gpYe1Vbhu"
      },
      "outputs": [],
      "source": [
        "# function to train the model\n",
        "def train(train_dataloader):\n",
        "  \n",
        "  model.train()\n",
        "\n",
        "  total_loss, total_accuracy = 0, 0\n",
        "  \n",
        "  # empty list to save model predictions\n",
        "  total_preds=[]\n",
        "  total, correct=0, 0\n",
        "  # iterate over batches\n",
        "  for step,batch in enumerate(train_dataloader):\n",
        "    \n",
        "    # progress update after every 50 batches.\n",
        "    if step % 50 == 0 and not step == 0:\n",
        "      print('  Batch {:>5,}  of  {:>5,}.'.format(step, len(train_dataloader)))\n",
        "\n",
        "    # push the batch to gpu\n",
        "    batch = [r.to(device) for r in batch]\n",
        " \n",
        "    sent_id, mask, labels = batch\n",
        "\n",
        "    # clear previously calculated gradients \n",
        "    model.zero_grad()        \n",
        "\n",
        "    # get model predictions for the current batch\n",
        "    preds = model(sent_id, mask)\n",
        "\n",
        "    # compute the loss between actual and predicted values\n",
        "    loss = cross_entropy(preds, labels)\n",
        "\n",
        "    total += labels.size(0)\n",
        "    _, predicted = torch.max(preds.data, 1)\n",
        "    correct += (predicted == labels).sum().item()\n",
        "\n",
        "    # add on to the total loss\n",
        "    total_loss = total_loss + loss.item()\n",
        "\n",
        "    # backward pass to calculate the gradients\n",
        "    loss.backward()\n",
        "\n",
        "    # clip the the gradients to 1.0. It helps in preventing the exploding gradient problem\n",
        "    torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "\n",
        "    # update parameters\n",
        "    optimizer.step()\n",
        "    scheduler.step()\n",
        "\n",
        "    # model predictions are stored on GPU. So, push it to CPU\n",
        "    preds=preds.detach().cpu().numpy()\n",
        "\n",
        "    # append the model predictions\n",
        "    total_preds.append(preds)\n",
        "\n",
        "  # compute the training loss of the epoch\n",
        "  avg_loss = total_loss / len(train_dataloader)\n",
        "  \n",
        "  # predictions are in the form of (no. of batches, size of batch, no. of classes).\n",
        "  # reshape the predictions in form of (number of samples, no. of classes)\n",
        "  total_preds  = np.concatenate(total_preds, axis=0)\n",
        "\n",
        "  acc = 100*correct/total\n",
        "  #returns the loss and predictions\n",
        "  return avg_loss, total_preds, acc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XmqZPiL_Vbhu"
      },
      "outputs": [],
      "source": [
        "# function for evaluating the model\n",
        "def evaluate(val_dataloader):\n",
        "  \n",
        "  print(\"\\nEvaluating...\")\n",
        "  \n",
        "  # deactivate dropout layers\n",
        "  model.eval()\n",
        "\n",
        "  total_loss, total_accuracy = 0, 0\n",
        "  \n",
        "  # empty list to save the model predictions\n",
        "  total_preds = []\n",
        "  total, correct=0, 0\n",
        "\n",
        "\n",
        "  # iterate over batches\n",
        "  for step,batch in enumerate(val_dataloader):\n",
        "    \n",
        "    # Progress update every 50 batches.\n",
        "    if step % 50 == 0 and not step == 0:\n",
        "            \n",
        "      # Report progress.\n",
        "      print('  Batch {:>5,}  of  {:>5,}.'.format(step, len(val_dataloader)))\n",
        "\n",
        "    # push the batch to gpu\n",
        "    batch = [t.to(device) for t in batch]\n",
        "\n",
        "    sent_id, mask, labels = batch\n",
        "\n",
        "    # deactivate autograd\n",
        "    with torch.no_grad():\n",
        "      \n",
        "      # model predictions\n",
        "      preds = model(sent_id, mask)\n",
        "\n",
        "      # compute the validation loss between actual and predicted values\n",
        "      loss = cross_entropy(preds,labels)\n",
        "\n",
        "      total += labels.size(0)\n",
        "      _, predicted = torch.max(preds.data, 1)\n",
        "      correct += (predicted == labels).sum().item()\n",
        "      total_loss = total_loss + loss.item()\n",
        "\n",
        "      preds = preds.detach().cpu().numpy()\n",
        "\n",
        "      total_preds.append(preds)\n",
        "\n",
        "  # compute the validation loss of the epoch\n",
        "  avg_loss = total_loss / len(val_dataloader) \n",
        "  preds = np.argmax(preds, axis = 1)\n",
        "  # reshape the predictions in form of (number of samples, no. of classes)\n",
        "  total_preds  = np.concatenate(total_preds, axis=0)\n",
        "  acc = 100*correct/total\n",
        "\n",
        "  return avg_loss, total_preds, acc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EkHuUQVMwc3V"
      },
      "outputs": [],
      "source": [
        "def train_model_on(train_dataloader, val_dataloader, epochs):\n",
        "  best_valid_loss = float('inf')\n",
        "\n",
        "  # empty lists to store training and validation loss of each epoch\n",
        "  train_losses=[]\n",
        "  valid_losses=[]\n",
        "\n",
        "  #for each epoch\n",
        "  for epoch in range(epochs):\n",
        "     \n",
        "      print('\\n Epoch {:} / {:}'.format(epoch + 1, epochs))\n",
        "    \n",
        "      #train model\n",
        "      train_loss, _, acc1 = train(train_dataloader)\n",
        "\n",
        "      #evaluate model\n",
        "      valid_loss, preds, acc2 = evaluate(val_dataloader)\n",
        "    \n",
        "\n",
        "      #save the best model\n",
        "      if valid_loss < best_valid_loss:\n",
        "        best_valid_loss = valid_loss\n",
        "        torch.save(model.state_dict(), 'saved_weights.pt')\n",
        "    \n",
        "    # append training and validation loss\n",
        "      train_losses.append(train_loss)\n",
        "      valid_losses.append(valid_loss)\n",
        "      print(f'\\nTraining Loss: {train_loss:.3f}')\n",
        "      print(f'Validation Loss: {valid_loss:.3f}')\n",
        "      print(f'Training Accuracy: {acc1:.3f}')\n",
        "      print(f'Validation Accuracy: {acc2:.3f}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5KrrkhfOhq5y"
      },
      "outputs": [],
      "source": [
        "def test_model_on(dataloader, test_y):\n",
        "  print(\"\\nTesting...\")\n",
        "  path = 'saved_weights.pt'\n",
        "  model.load_state_dict(torch.load(path))\n",
        "  print(\"Model state loaded from file \"+path)\n",
        "  total_loss, total_accuracy = 0, 0\n",
        "  \n",
        "  # empty list to save the model predictions\n",
        "  total_preds = []\n",
        "  total, correct=0, 0\n",
        "\n",
        "  # iterate over batches\n",
        "  for step,batch in enumerate(dataloader):\n",
        "    \n",
        "    # Progress update every 50 batches.\n",
        "    if step % 50 == 0 and not step == 0 :\n",
        "            \n",
        "      # Report progress.\n",
        "      print('  Batch {:>5,}  of  {:>5,}.'.format(step, len(dataloader)))\n",
        "\n",
        "    # push the batch to gpu\n",
        "    batch = [t.to(device) for t in batch]\n",
        "\n",
        "    sent_id, mask, labels = batch\n",
        "\n",
        "    # deactivate autograd\n",
        "    with torch.no_grad():\n",
        "      \n",
        "      # model predictions\n",
        "      preds = model(sent_id, mask)\n",
        "      total += labels.size(0)\n",
        "      _, predicted = torch.max(preds.data, 1)\n",
        "      correct += (predicted == labels).sum().item()\n",
        "      predicted = predicted.detach().cpu().numpy()\n",
        "      total_preds.append(predicted)\n",
        "      \n",
        "  \n",
        "  total_preds  = np.concatenate(total_preds, axis=0)\n",
        "  test_y = [i+1 for i in test_y]\n",
        "  total_preds = [i+1 for i in total_preds]\n",
        "  acc = 100*correct/total\n",
        "  MAE = mean_absolute_error(test_y, total_preds)\n",
        "  report = classification_report(test_y, total_preds)\n",
        "  return acc, MAE, report"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l_RwwVfKuxlu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "83c57e1d-365d-40b2-b53f-6305c750f723"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " Epoch 1 / 10\n",
            "  Batch    50  of  1,094.\n",
            "  Batch   100  of  1,094.\n",
            "  Batch   150  of  1,094.\n",
            "  Batch   200  of  1,094.\n",
            "  Batch   250  of  1,094.\n",
            "  Batch   300  of  1,094.\n",
            "  Batch   350  of  1,094.\n",
            "  Batch   400  of  1,094.\n",
            "  Batch   450  of  1,094.\n",
            "  Batch   500  of  1,094.\n",
            "  Batch   550  of  1,094.\n",
            "  Batch   600  of  1,094.\n",
            "  Batch   650  of  1,094.\n",
            "  Batch   700  of  1,094.\n",
            "  Batch   750  of  1,094.\n",
            "  Batch   800  of  1,094.\n",
            "  Batch   850  of  1,094.\n",
            "  Batch   900  of  1,094.\n",
            "  Batch   950  of  1,094.\n",
            "  Batch 1,000  of  1,094.\n",
            "  Batch 1,050  of  1,094.\n",
            "\n",
            "Evaluating...\n",
            "  Batch    50  of    469.\n",
            "  Batch   100  of    469.\n",
            "  Batch   150  of    469.\n",
            "  Batch   200  of    469.\n",
            "  Batch   250  of    469.\n",
            "  Batch   300  of    469.\n",
            "  Batch   350  of    469.\n",
            "  Batch   400  of    469.\n",
            "  Batch   450  of    469.\n",
            "\n",
            "Training Loss: 1.256\n",
            "Validation Loss: 1.121\n",
            "Training Accuracy: 43.114\n",
            "Validation Accuracy: 50.053\n",
            "\n",
            " Epoch 2 / 10\n",
            "  Batch    50  of  1,094.\n",
            "  Batch   100  of  1,094.\n",
            "  Batch   150  of  1,094.\n",
            "  Batch   200  of  1,094.\n",
            "  Batch   250  of  1,094.\n",
            "  Batch   300  of  1,094.\n",
            "  Batch   350  of  1,094.\n",
            "  Batch   400  of  1,094.\n",
            "  Batch   450  of  1,094.\n",
            "  Batch   500  of  1,094.\n",
            "  Batch   550  of  1,094.\n",
            "  Batch   600  of  1,094.\n",
            "  Batch   650  of  1,094.\n",
            "  Batch   700  of  1,094.\n",
            "  Batch   750  of  1,094.\n",
            "  Batch   800  of  1,094.\n",
            "  Batch   850  of  1,094.\n",
            "  Batch   900  of  1,094.\n",
            "  Batch   950  of  1,094.\n",
            "  Batch 1,000  of  1,094.\n",
            "  Batch 1,050  of  1,094.\n",
            "\n",
            "Evaluating...\n",
            "  Batch    50  of    469.\n",
            "  Batch   100  of    469.\n",
            "  Batch   150  of    469.\n",
            "  Batch   200  of    469.\n",
            "  Batch   250  of    469.\n",
            "  Batch   300  of    469.\n",
            "  Batch   350  of    469.\n",
            "  Batch   400  of    469.\n",
            "  Batch   450  of    469.\n",
            "\n",
            "Training Loss: 1.041\n",
            "Validation Loss: 1.087\n",
            "Training Accuracy: 53.863\n",
            "Validation Accuracy: 52.307\n",
            "\n",
            " Epoch 3 / 10\n",
            "  Batch    50  of  1,094.\n",
            "  Batch   100  of  1,094.\n",
            "  Batch   150  of  1,094.\n",
            "  Batch   200  of  1,094.\n",
            "  Batch   250  of  1,094.\n",
            "  Batch   300  of  1,094.\n",
            "  Batch   350  of  1,094.\n",
            "  Batch   400  of  1,094.\n",
            "  Batch   450  of  1,094.\n",
            "  Batch   500  of  1,094.\n",
            "  Batch   550  of  1,094.\n",
            "  Batch   600  of  1,094.\n",
            "  Batch   650  of  1,094.\n",
            "  Batch   700  of  1,094.\n",
            "  Batch   750  of  1,094.\n",
            "  Batch   800  of  1,094.\n",
            "  Batch   850  of  1,094.\n",
            "  Batch   900  of  1,094.\n",
            "  Batch   950  of  1,094.\n",
            "  Batch 1,000  of  1,094.\n",
            "  Batch 1,050  of  1,094.\n",
            "\n",
            "Evaluating...\n",
            "  Batch    50  of    469.\n",
            "  Batch   100  of    469.\n",
            "  Batch   150  of    469.\n",
            "  Batch   200  of    469.\n",
            "  Batch   250  of    469.\n",
            "  Batch   300  of    469.\n",
            "  Batch   350  of    469.\n",
            "  Batch   400  of    469.\n",
            "  Batch   450  of    469.\n",
            "\n",
            "Training Loss: 0.924\n",
            "Validation Loss: 1.110\n",
            "Training Accuracy: 60.080\n",
            "Validation Accuracy: 52.720\n",
            "\n",
            " Epoch 4 / 10\n",
            "  Batch    50  of  1,094.\n",
            "  Batch   100  of  1,094.\n",
            "  Batch   150  of  1,094.\n",
            "  Batch   200  of  1,094.\n",
            "  Batch   250  of  1,094.\n",
            "  Batch   300  of  1,094.\n",
            "  Batch   350  of  1,094.\n",
            "  Batch   400  of  1,094.\n",
            "  Batch   450  of  1,094.\n",
            "  Batch   500  of  1,094.\n",
            "  Batch   550  of  1,094.\n",
            "  Batch   600  of  1,094.\n",
            "  Batch   650  of  1,094.\n",
            "  Batch   700  of  1,094.\n",
            "  Batch   750  of  1,094.\n",
            "  Batch   800  of  1,094.\n",
            "  Batch   850  of  1,094.\n",
            "  Batch   900  of  1,094.\n",
            "  Batch   950  of  1,094.\n",
            "  Batch 1,000  of  1,094.\n",
            "  Batch 1,050  of  1,094.\n",
            "\n",
            "Evaluating...\n",
            "  Batch    50  of    469.\n",
            "  Batch   100  of    469.\n",
            "  Batch   150  of    469.\n",
            "  Batch   200  of    469.\n",
            "  Batch   250  of    469.\n",
            "  Batch   300  of    469.\n",
            "  Batch   350  of    469.\n",
            "  Batch   400  of    469.\n",
            "  Batch   450  of    469.\n",
            "\n",
            "Training Loss: 0.804\n",
            "Validation Loss: 1.197\n",
            "Training Accuracy: 66.457\n",
            "Validation Accuracy: 52.107\n",
            "\n",
            " Epoch 5 / 10\n",
            "  Batch    50  of  1,094.\n",
            "  Batch   100  of  1,094.\n",
            "  Batch   150  of  1,094.\n",
            "  Batch   200  of  1,094.\n",
            "  Batch   250  of  1,094.\n",
            "  Batch   300  of  1,094.\n",
            "  Batch   350  of  1,094.\n",
            "  Batch   400  of  1,094.\n",
            "  Batch   450  of  1,094.\n",
            "  Batch   500  of  1,094.\n",
            "  Batch   550  of  1,094.\n",
            "  Batch   600  of  1,094.\n",
            "  Batch   650  of  1,094.\n",
            "  Batch   700  of  1,094.\n",
            "  Batch   750  of  1,094.\n",
            "  Batch   800  of  1,094.\n",
            "  Batch   850  of  1,094.\n",
            "  Batch   900  of  1,094.\n",
            "  Batch   950  of  1,094.\n",
            "  Batch 1,000  of  1,094.\n",
            "  Batch 1,050  of  1,094.\n",
            "\n",
            "Evaluating...\n",
            "  Batch    50  of    469.\n",
            "  Batch   100  of    469.\n",
            "  Batch   150  of    469.\n",
            "  Batch   200  of    469.\n",
            "  Batch   250  of    469.\n",
            "  Batch   300  of    469.\n",
            "  Batch   350  of    469.\n",
            "  Batch   400  of    469.\n",
            "  Batch   450  of    469.\n",
            "\n",
            "Training Loss: 0.683\n",
            "Validation Loss: 1.305\n",
            "Training Accuracy: 72.674\n",
            "Validation Accuracy: 50.853\n",
            "\n",
            " Epoch 6 / 10\n",
            "  Batch    50  of  1,094.\n",
            "  Batch   100  of  1,094.\n",
            "  Batch   150  of  1,094.\n",
            "  Batch   200  of  1,094.\n",
            "  Batch   250  of  1,094.\n",
            "  Batch   300  of  1,094.\n",
            "  Batch   350  of  1,094.\n",
            "  Batch   400  of  1,094.\n",
            "  Batch   450  of  1,094.\n",
            "  Batch   500  of  1,094.\n",
            "  Batch   550  of  1,094.\n",
            "  Batch   600  of  1,094.\n",
            "  Batch   650  of  1,094.\n",
            "  Batch   700  of  1,094.\n",
            "  Batch   750  of  1,094.\n",
            "  Batch   800  of  1,094.\n",
            "  Batch   850  of  1,094.\n",
            "  Batch   900  of  1,094.\n",
            "  Batch   950  of  1,094.\n",
            "  Batch 1,000  of  1,094.\n",
            "  Batch 1,050  of  1,094.\n",
            "\n",
            "Evaluating...\n",
            "  Batch    50  of    469.\n",
            "  Batch   100  of    469.\n",
            "  Batch   150  of    469.\n",
            "  Batch   200  of    469.\n",
            "  Batch   250  of    469.\n",
            "  Batch   300  of    469.\n",
            "  Batch   350  of    469.\n",
            "  Batch   400  of    469.\n",
            "  Batch   450  of    469.\n",
            "\n",
            "Training Loss: 0.566\n",
            "Validation Loss: 1.452\n",
            "Training Accuracy: 78.331\n",
            "Validation Accuracy: 50.907\n",
            "\n",
            " Epoch 7 / 10\n",
            "  Batch    50  of  1,094.\n",
            "  Batch   100  of  1,094.\n",
            "  Batch   150  of  1,094.\n",
            "  Batch   200  of  1,094.\n",
            "  Batch   250  of  1,094.\n",
            "  Batch   300  of  1,094.\n",
            "  Batch   350  of  1,094.\n",
            "  Batch   400  of  1,094.\n",
            "  Batch   450  of  1,094.\n",
            "  Batch   500  of  1,094.\n",
            "  Batch   550  of  1,094.\n",
            "  Batch   600  of  1,094.\n",
            "  Batch   650  of  1,094.\n",
            "  Batch   700  of  1,094.\n",
            "  Batch   750  of  1,094.\n",
            "  Batch   800  of  1,094.\n",
            "  Batch   850  of  1,094.\n",
            "  Batch   900  of  1,094.\n",
            "  Batch   950  of  1,094.\n",
            "  Batch 1,000  of  1,094.\n",
            "  Batch 1,050  of  1,094.\n",
            "\n",
            "Evaluating...\n",
            "  Batch    50  of    469.\n",
            "  Batch   100  of    469.\n",
            "  Batch   150  of    469.\n",
            "  Batch   200  of    469.\n",
            "  Batch   250  of    469.\n",
            "  Batch   300  of    469.\n",
            "  Batch   350  of    469.\n",
            "  Batch   400  of    469.\n",
            "  Batch   450  of    469.\n",
            "\n",
            "Training Loss: 0.466\n",
            "Validation Loss: 1.580\n",
            "Training Accuracy: 82.851\n",
            "Validation Accuracy: 51.120\n",
            "\n",
            " Epoch 8 / 10\n",
            "  Batch    50  of  1,094.\n",
            "  Batch   100  of  1,094.\n",
            "  Batch   150  of  1,094.\n",
            "  Batch   200  of  1,094.\n",
            "  Batch   250  of  1,094.\n",
            "  Batch   300  of  1,094.\n",
            "  Batch   350  of  1,094.\n",
            "  Batch   400  of  1,094.\n",
            "  Batch   450  of  1,094.\n",
            "  Batch   500  of  1,094.\n",
            "  Batch   550  of  1,094.\n",
            "  Batch   600  of  1,094.\n",
            "  Batch   650  of  1,094.\n",
            "  Batch   700  of  1,094.\n",
            "  Batch   750  of  1,094.\n",
            "  Batch   800  of  1,094.\n",
            "  Batch   850  of  1,094.\n",
            "  Batch   900  of  1,094.\n",
            "  Batch   950  of  1,094.\n",
            "  Batch 1,000  of  1,094.\n",
            "  Batch 1,050  of  1,094.\n",
            "\n",
            "Evaluating...\n",
            "  Batch    50  of    469.\n",
            "  Batch   100  of    469.\n",
            "  Batch   150  of    469.\n",
            "  Batch   200  of    469.\n",
            "  Batch   250  of    469.\n",
            "  Batch   300  of    469.\n",
            "  Batch   350  of    469.\n",
            "  Batch   400  of    469.\n",
            "  Batch   450  of    469.\n",
            "\n",
            "Training Loss: 0.377\n",
            "Validation Loss: 1.763\n",
            "Training Accuracy: 86.829\n",
            "Validation Accuracy: 50.027\n",
            "\n",
            " Epoch 9 / 10\n",
            "  Batch    50  of  1,094.\n",
            "  Batch   100  of  1,094.\n",
            "  Batch   150  of  1,094.\n",
            "  Batch   200  of  1,094.\n",
            "  Batch   250  of  1,094.\n",
            "  Batch   300  of  1,094.\n",
            "  Batch   350  of  1,094.\n",
            "  Batch   400  of  1,094.\n",
            "  Batch   450  of  1,094.\n",
            "  Batch   500  of  1,094.\n",
            "  Batch   550  of  1,094.\n",
            "  Batch   600  of  1,094.\n",
            "  Batch   650  of  1,094.\n",
            "  Batch   700  of  1,094.\n",
            "  Batch   750  of  1,094.\n",
            "  Batch   800  of  1,094.\n",
            "  Batch   850  of  1,094.\n",
            "  Batch   900  of  1,094.\n",
            "  Batch   950  of  1,094.\n",
            "  Batch 1,000  of  1,094.\n",
            "  Batch 1,050  of  1,094.\n",
            "\n",
            "Evaluating...\n",
            "  Batch    50  of    469.\n",
            "  Batch   100  of    469.\n",
            "  Batch   150  of    469.\n",
            "  Batch   200  of    469.\n",
            "  Batch   250  of    469.\n",
            "  Batch   300  of    469.\n",
            "  Batch   350  of    469.\n",
            "  Batch   400  of    469.\n",
            "  Batch   450  of    469.\n",
            "\n",
            "Training Loss: 0.320\n",
            "Validation Loss: 1.863\n",
            "Training Accuracy: 89.251\n",
            "Validation Accuracy: 50.173\n",
            "\n",
            " Epoch 10 / 10\n",
            "  Batch    50  of  1,094.\n",
            "  Batch   100  of  1,094.\n",
            "  Batch   150  of  1,094.\n",
            "  Batch   200  of  1,094.\n",
            "  Batch   250  of  1,094.\n",
            "  Batch   300  of  1,094.\n",
            "  Batch   350  of  1,094.\n",
            "  Batch   400  of  1,094.\n",
            "  Batch   450  of  1,094.\n",
            "  Batch   500  of  1,094.\n",
            "  Batch   550  of  1,094.\n",
            "  Batch   600  of  1,094.\n",
            "  Batch   650  of  1,094.\n",
            "  Batch   700  of  1,094.\n",
            "  Batch   750  of  1,094.\n",
            "  Batch   800  of  1,094.\n",
            "  Batch   850  of  1,094.\n",
            "  Batch   900  of  1,094.\n",
            "  Batch   950  of  1,094.\n",
            "  Batch 1,000  of  1,094.\n",
            "  Batch 1,050  of  1,094.\n",
            "\n",
            "Evaluating...\n",
            "  Batch    50  of    469.\n",
            "  Batch   100  of    469.\n",
            "  Batch   150  of    469.\n",
            "  Batch   200  of    469.\n",
            "  Batch   250  of    469.\n",
            "  Batch   300  of    469.\n",
            "  Batch   350  of    469.\n",
            "  Batch   400  of    469.\n",
            "  Batch   450  of    469.\n",
            "\n",
            "Training Loss: 0.267\n",
            "Validation Loss: 1.941\n",
            "Training Accuracy: 91.383\n",
            "Validation Accuracy: 50.373\n"
          ]
        }
      ],
      "source": [
        "train_model_on(train_dataloader, val_dataloader, epochs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GI9Kg4qwiJL-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0f7c76cd-57cc-4e35-c96a-32a1941547ab"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Start running the model on English\n",
            "\n",
            "Testing...\n",
            "Model state loaded from file saved_weights.pt\n",
            "  Batch    50  of    313.\n",
            "  Batch   100  of    313.\n",
            "  Batch   150  of    313.\n",
            "  Batch   200  of    313.\n",
            "  Batch   250  of    313.\n",
            "  Batch   300  of    313.\n",
            "Test Accuracy: 47.620\n",
            "MAE: 0.703\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           1       0.55      0.63      0.59      1000\n",
            "           2       0.39      0.36      0.38      1000\n",
            "           3       0.35      0.44      0.39      1000\n",
            "           4       0.45      0.32      0.37      1000\n",
            "           5       0.66      0.62      0.64      1000\n",
            "\n",
            "    accuracy                           0.48      5000\n",
            "   macro avg       0.48      0.48      0.47      5000\n",
            "weighted avg       0.48      0.48      0.47      5000\n",
            "\n"
          ]
        }
      ],
      "source": [
        "print(\"Start running the model on English\")\n",
        "test_dataloader, test_y=convert_test_data_of('en')\n",
        "acc3, mae, report=test_model_on(test_dataloader, test_y)\n",
        "print(f'Test Accuracy: {acc3:.3f}')\n",
        "print(f'MAE: {mae:.3f}')\n",
        "print(report)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qhtTdpxCqGaY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9355dbc6-9dda-443e-a930-d7f22d304c75"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Start running the model on French\n",
            "\n",
            "Testing...\n",
            "Model state loaded from file saved_weights.pt\n",
            "  Batch    50  of    313.\n",
            "  Batch   100  of    313.\n",
            "  Batch   150  of    313.\n",
            "  Batch   200  of    313.\n",
            "  Batch   250  of    313.\n",
            "  Batch   300  of    313.\n",
            "Test Accuracy: 52.360\n",
            "MAE: 0.589\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           1       0.60      0.78      0.67      1000\n",
            "           2       0.48      0.34      0.40      1000\n",
            "           3       0.43      0.43      0.43      1000\n",
            "           4       0.45      0.47      0.46      1000\n",
            "           5       0.64      0.59      0.62      1000\n",
            "\n",
            "    accuracy                           0.52      5000\n",
            "   macro avg       0.52      0.52      0.52      5000\n",
            "weighted avg       0.52      0.52      0.52      5000\n",
            "\n"
          ]
        }
      ],
      "source": [
        "print(\"Start running the model on French\")\n",
        "test_dataloader, test_y=convert_test_data_of('fr')\n",
        "acc, mae, report=test_model_on(test_dataloader, test_y)\n",
        "print(f'Test Accuracy: {acc:.3f}')\n",
        "print(f'MAE: {mae:.3f}')\n",
        "print(report)\n",
        "# print(test_dataloader)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hjM7MiZ8rS3x",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7196c4de-f341-4acf-c1a3-3f10bd093edb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Start running the model on Germany\n",
            "\n",
            "Testing...\n",
            "Model state loaded from file saved_weights.pt\n",
            "  Batch    50  of    313.\n",
            "  Batch   100  of    313.\n",
            "  Batch   150  of    313.\n",
            "  Batch   200  of    313.\n",
            "  Batch   250  of    313.\n",
            "  Batch   300  of    313.\n",
            "Test Accuracy: 43.260\n",
            "MAE: 0.781\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           1       0.59      0.45      0.51      1000\n",
            "           2       0.36      0.36      0.36      1000\n",
            "           3       0.33      0.55      0.41      1000\n",
            "           4       0.39      0.32      0.35      1000\n",
            "           5       0.66      0.48      0.56      1000\n",
            "\n",
            "    accuracy                           0.43      5000\n",
            "   macro avg       0.46      0.43      0.44      5000\n",
            "weighted avg       0.46      0.43      0.44      5000\n",
            "\n"
          ]
        }
      ],
      "source": [
        "print(\"Start running the model on Germany\")\n",
        "test_dataloader, test_y=convert_test_data_of('de')\n",
        "acc, mae, report=test_model_on(test_dataloader, test_y)\n",
        "print(f'Test Accuracy: {acc:.3f}')\n",
        "print(f'MAE: {mae:.3f}')\n",
        "print(report)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jUc1spFGsapc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "45a597ee-4946-4f91-9895-d19ccdc20418"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Start running the model on Spanish\n",
            "\n",
            "Testing...\n",
            "Model state loaded from file saved_weights.pt\n",
            "  Batch    50  of    313.\n",
            "  Batch   100  of    313.\n",
            "  Batch   150  of    313.\n",
            "  Batch   200  of    313.\n",
            "  Batch   250  of    313.\n",
            "  Batch   300  of    313.\n",
            "Test Accuracy: 44.760\n",
            "MAE: 0.691\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           1       0.70      0.45      0.55      1000\n",
            "           2       0.38      0.30      0.33      1000\n",
            "           3       0.34      0.51      0.41      1000\n",
            "           4       0.40      0.56      0.46      1000\n",
            "           5       0.66      0.41      0.51      1000\n",
            "\n",
            "    accuracy                           0.45      5000\n",
            "   macro avg       0.49      0.45      0.45      5000\n",
            "weighted avg       0.49      0.45      0.45      5000\n",
            "\n"
          ]
        }
      ],
      "source": [
        "print(\"Start running the model on Spanish\")\n",
        "test_dataloader, test_y=convert_test_data_of('es')\n",
        "acc, mae, report=test_model_on(test_dataloader, test_y)\n",
        "print(f'Test Accuracy: {acc:.3f}')\n",
        "print(f'MAE: {mae:.3f}')\n",
        "print(report)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N6q8yAnIsPbk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e74cb248-d48f-458f-81bc-67df0f09292a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Start running the model on Japanese\n",
            "\n",
            "Testing...\n",
            "Model state loaded from file saved_weights.pt\n",
            "  Batch    50  of    313.\n",
            "  Batch   100  of    313.\n",
            "  Batch   150  of    313.\n",
            "  Batch   200  of    313.\n",
            "  Batch   250  of    313.\n",
            "  Batch   300  of    313.\n",
            "Test Accuracy: 33.080\n",
            "MAE: 1.171\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           1       0.35      0.68      0.46      1000\n",
            "           2       0.26      0.32      0.29      1000\n",
            "           3       0.27      0.31      0.29      1000\n",
            "           4       0.32      0.07      0.12      1000\n",
            "           5       0.57      0.27      0.37      1000\n",
            "\n",
            "    accuracy                           0.33      5000\n",
            "   macro avg       0.36      0.33      0.31      5000\n",
            "weighted avg       0.36      0.33      0.31      5000\n",
            "\n"
          ]
        }
      ],
      "source": [
        "print(\"Start running the model on Japanese\")\n",
        "test_dataloader, test_y=convert_test_data_of('ja')\n",
        "acc, mae, report=test_model_on(test_dataloader, test_y)\n",
        "print(f'Test Accuracy: {acc:.3f}')\n",
        "print(f'MAE: {mae:.3f}')\n",
        "print(report)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C2rkpWLxsPqU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d230c6e2-05f6-400b-e0a2-fa9422ad6472"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Start running the model on Chinese\n",
            "\n",
            "Testing...\n",
            "Model state loaded from file saved_weights.pt\n",
            "  Batch    50  of    313.\n",
            "  Batch   100  of    313.\n",
            "  Batch   150  of    313.\n",
            "  Batch   200  of    313.\n",
            "  Batch   250  of    313.\n",
            "  Batch   300  of    313.\n",
            "Test Accuracy: 34.700\n",
            "MAE: 0.947\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           1       0.50      0.52      0.51      1000\n",
            "           2       0.30      0.40      0.34      1000\n",
            "           3       0.27      0.46      0.34      1000\n",
            "           4       0.28      0.19      0.23      1000\n",
            "           5       0.65      0.17      0.27      1000\n",
            "\n",
            "    accuracy                           0.35      5000\n",
            "   macro avg       0.40      0.35      0.34      5000\n",
            "weighted avg       0.40      0.35      0.34      5000\n",
            "\n"
          ]
        }
      ],
      "source": [
        "print(\"Start running the model on Chinese\")\n",
        "test_dataloader, test_y=convert_test_data_of('zh')\n",
        "acc, mae, report=test_model_on(test_dataloader, test_y)\n",
        "print(f'Test Accuracy: {acc:.3f}')\n",
        "print(f'MAE: {mae:.3f}')\n",
        "print(report)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "machine_shape": "hm",
      "name": "mbert_fr.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}